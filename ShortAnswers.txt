Part 1.1:

P(Sam | am) = (c(w) + 1) / (N + V)
V = {<s>, </s>, I, am, Sam, do, not, like, green, eggs, and} => |V| = 11
N = 3
c(w) = 2
P(Sam | am) = (2 + 1) / (3 + 11) = 3 / 14 

Ans: 3/14

Section 1.3

Question 1:
The total number of unique words in the training corpus after processing is 41738.

Question 2:
The total number of word tokens in the training corpus exclude <cs> is 2520268.

Question 3:
The percentage of word tokens and word types in the test corpus that did not occur in training before mapping 
unknown words to <unk> is as follows:

Token Percentage: 1.95%
Type Percentage: 3.87%

Question 4: 

The percentage of Bigram Types and Bigram Tokens are:

Bigram Token Percentage: 24.17%
Bigram Type Percentage: 27.72%

Question 5:

-94.9393 Unigram 
-38.8685 Bigram
-55.7257 Add-One Smoothing

Probability Calculations and parameters:
Unigram: P(w) = c(w) / totalWords

for key in words_dict:
    probability = round(words_dict[key] / totalWords, 8)

Bigram: P(w2|w1) = c(w1, w2) / c(w1)

for key in bigram_dict:
    probability = round(bigram_dict[key] / word_dict[key[0]], 8)

Add One Smoothing: P(w2|w1) = 1 + c(w1, w2) / uniqueWords + c(w1)

probability = round((1.0 + bigram_dict[key]) / (uniqueWords + word_dict[key[0]]), 8)

In order to identify zero values, we account for all unigram, bigram and add one that do not appear in the training data.

For Unigram, we would use:
zero_unigram = [word for word, count in words_dict.items() if count == 0]

For Bigram:
zero_bigram = [bigram for bigram in bigram_dict if bigram_dict[bigram] == 0]

And for Add-One Smoothing:
zero_add_one = [bigram for bigram in bigram_dict if bigram_dict[bigram] == 0]

To do the complete calculation, 

# Assuming 'words_dict' and 'totalWords' are defined
unigram_model = unigram_model(words_dict, totalTokenCount)
print("Unigram Model Probabilities:")
writeDict(unigram_model, outputFile)

# For bigram model
bigram_dict = create_bigram(processedtrainFile)
bigram_model = bigram_model(bigram_dict, words_dict)
print("Bigram Model Probabilities:")
writeDict(bigram_model, outputFile)

# For Add-One Smoothing
add_one_smoothing = add_one_smoothing(bigram_dict, words_dict, uniqueTokens)
print("Add-One Smoothed Bigram Model Probabilities:")
writeDict(add_one_smoothing, outputFile)

# Identifying zero counts
zero_unigram = compare_to(words_dict, set(words_dict.keys()))
zero_bigram = compare_to(bigram_dict, set(bigram_dict.keys()))
zero_add_one = compare_to(add_one_smoothing, set(add_one_smoothing.keys()))

Question 6:

Perplexity:
13278.8789 Unigram
48.7571 Bigram
263.1094 Add-One Smoothing

Question 7:

1.0112 Unigram
1.0056 Bigram
1.0 Add One Smoothing